{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQu7vKTjJdvh"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip install transformers==4.44.2 torch>=2.0.0 torchvision>=0.15.0 ultralytics>=8.0.0 opencv-python>=4.8.0 numpy>=1.24.0 pillow>=9.0.0 accelerate>=0.20.0 huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"insert-token\")  # replace with actual hf token"
      ],
      "metadata": {
        "id": "BU34vrTIKTNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p human-centric-video-stabilization/src"
      ],
      "metadata": {
        "id": "zfeYspVAKmnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile human-centric-video-stabilization/requirements.txt\n",
        "torch>=2.0.0\n",
        "torchvision>=0.15.0\n",
        "transformers==4.44.2\n",
        "ultralytics>=8.0.0\n",
        "opencv-python>=4.8.0\n",
        "numpy>=1.24.0\n",
        "pillow>=9.0.0\n",
        "accelerate>=0.20.0\n",
        "huggingface-hub\n",
        "scipy>=1.10.0"
      ],
      "metadata": {
        "id": "n3q-4QBBKmp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile human-centric-video-stabilization/src/__init__.py\n",
        "# Empty file to make src a package"
      ],
      "metadata": {
        "id": "ima7Kt74KmsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile human-centric-video-stabilization/src/background.py\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import AutoModelForImageSegmentation\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # suppress minor warnings\n",
        "\n",
        "def load_background_remover(device='cpu'):\n",
        "    \"\"\"\n",
        "    loading the RMBG-2.0 model for background removal.\n",
        "    requires Hugging Face authentication for gated model.\n",
        "    returns: model, transform_image\n",
        "    \"\"\"\n",
        "    # setting precision for efficiency (as per model docs)\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # loading model WITHOUT dtype (causes TypeError in BiRefNet.__init__)\n",
        "    model = AutoModelForImageSegmentation.from_pretrained(\n",
        "        'briaai/RMBG-2.0',\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # moving to device (keeping float32 on GPU for compatibility)\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # image size for model input (as per model docs)\n",
        "    image_size = (1024, 1024)\n",
        "\n",
        "    # preprocessing transform (ImageNet normalization, as per model docs)\n",
        "    transform_image = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return model, transform_image\n",
        "\n",
        "def remove_background(frame, model, transform_image):\n",
        "    \"\"\"\n",
        "    remove background from a BGR frame using RMBG-2.0.\n",
        "    returns RGBA numpy array (BGR + Alpha), where alpha is soft (0-255) from model probability.\n",
        "    \"\"\"\n",
        "    if frame is None or frame.size == 0:\n",
        "        raise ValueError(\"Invalid frame input\")\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # converting BGR to RGB for model\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pil_rgb = Image.fromarray(rgb)\n",
        "    original_size = pil_rgb.size  # (W, H)\n",
        "\n",
        "    # preprocessing\n",
        "    input_tensor = transform_image(pil_rgb).unsqueeze(0).to(device)\n",
        "\n",
        "    # inference (as per model docs: model(input)[-1].sigmoid())\n",
        "    with torch.no_grad():\n",
        "        pred = model(input_tensor)[-1].sigmoid().cpu()[0].squeeze()  # (H=1024, W=1024), 0-1 prob\n",
        "\n",
        "    # post-processing: Convert to PIL grayscale (0-255), resize to original (LANCZOS for quality)\n",
        "    pred_pil = transforms.ToPILImage()(pred)\n",
        "    mask_pil = pred_pil.resize(original_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    # alpha mask as uint8\n",
        "    alpha = np.array(mask_pil)  # (H, W), 0-255\n",
        "\n",
        "    # stacking BGR + Alpha\n",
        "    rgba = np.dstack([frame, alpha])\n",
        "\n",
        "    return rgba"
      ],
      "metadata": {
        "id": "5AnAdwVaKmu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using only high-confidence keypoints)\n",
        "%%writefile human-centric-video-stabilization/src/pose.py\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def load_pose_model(model_path='yolov8n-pose.pt', device='cpu'):\n",
        "    \"\"\"\n",
        "    loading YOLOv8-Pose model.\n",
        "    \"\"\"\n",
        "    model = YOLO(model_path)\n",
        "    model.to(device)  # moving to device always (works for CPU too)\n",
        "    return model\n",
        "\n",
        "def detect_pose_center(model, frame):\n",
        "    results = model.track(frame, persist=True, verbose=False)  # enabling tracking\n",
        "    if len(results) == 0 or results[0].keypoints is None:\n",
        "        return None, []\n",
        "\n",
        "    # assuming main person is the one with ID 0 or lowest ID; filter if multi-person\n",
        "    keypoints = results[0].keypoints\n",
        "    if len(keypoints.xy) == 0:\n",
        "        return None, []\n",
        "\n",
        "    xy = keypoints.xy[0].cpu().numpy()\n",
        "    conf = keypoints.conf[0].cpu().numpy() if keypoints.conf is not None else np.ones(17)\n",
        "    keypoints_data = [(float(xy[i][0]), float(xy[i][1]), float(conf[i])) for i in range(17)]\n",
        "\n",
        "    # using average of high-confidence hips and shoulders (indices: 5/6 shoulders, 11/12 hips)\n",
        "    indices = [5, 6, 11, 12]\n",
        "    points = xy[indices]\n",
        "    confs = conf[indices]\n",
        "    valid_points = points[confs > 0.5]\n",
        "    if len(valid_points) > 0:\n",
        "        center_x = int(np.mean(valid_points[:, 0]))\n",
        "        center_y = int(np.mean(valid_points[:, 1]))\n",
        "        return (center_x, center_y), keypoints_data\n",
        "    return None, keypoints_data"
      ],
      "metadata": {
        "id": "lJ_zq37PKmxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile human-centric-video-stabilization/src/stabilization.py\n",
        "import numpy as np\n",
        "from scipy.linalg import block_diag  # for multi-dimensional Kalman\n",
        "\n",
        "class KalmanFilter:\n",
        "    def __init__(self, process_noise=0.1, measurement_noise=1.0):\n",
        "        # state: [x, y, vx, vy] (position + velocity)\n",
        "        self.state = np.zeros(4)  # initial state\n",
        "        self.P = np.eye(4) * 1e-3  # covariance\n",
        "        self.Q = block_diag(process_noise * np.eye(2), process_noise * np.eye(2))  # processing noise\n",
        "        self.R = measurement_noise * np.eye(2)  # measuring noise\n",
        "        self.F = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]])  # transition\n",
        "        self.H = np.array([[1, 0, 0, 0], [0, 1, 0, 0]])  # measurement\n",
        "\n",
        "    def update(self, measurement):\n",
        "        if measurement is None:\n",
        "            # predicting only if no measurement\n",
        "            self.state = self.F @ self.state\n",
        "            self.P = self.F @ self.P @ self.F.T + self.Q\n",
        "            return self.state[:2]\n",
        "\n",
        "        # predict\n",
        "        self.state = self.F @ self.state\n",
        "        self.P = self.F @ self.P @ self.F.T + self.Q\n",
        "\n",
        "        # update\n",
        "        y = measurement - self.H @ self.state\n",
        "        S = self.H @ self.P @ self.H.T + self.R\n",
        "        K = self.P @ self.H.T @ np.linalg.inv(S)\n",
        "        self.state += K @ y\n",
        "        self.P = (np.eye(4) - K @ self.H) @ self.P\n",
        "        return self.state[:2]\n",
        "\n",
        "def compute_transform(center_point, target_point, kalman):\n",
        "    if center_point is not None:\n",
        "        center_point = np.array(center_point, dtype=np.float32)\n",
        "    smoothed_center = kalman.update(center_point)\n",
        "    dx = target_point[0] - smoothed_center[0]\n",
        "    dy = target_point[1] - smoothed_center[1]\n",
        "    return np.array([dx, dy], dtype=np.float32)  # Kalman handles smoothing"
      ],
      "metadata": {
        "id": "dhpQsmb-Kmz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (adding border handling in warping to avoid black edges; transform keypoints for correct visualization on stabilized frame)\n",
        "%%writefile human-centric-video-stabilization/src/rendering.py\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def create_stabilized_frame(frame, foreground_rgba, current_transform, blur_bg=False, blur_kernel=15, visualize=False, keypoints_data=[], target_point=(0,0)):\n",
        "    \"\"\"\n",
        "    create stabilized frame:\n",
        "    - warp original frame by inverse transform (background drifts).\n",
        "    - warp foreground by forward transform (subject fixed).\n",
        "    - alpha blend.\n",
        "    returns: stabilized BGR frame.\n",
        "    \"\"\"\n",
        "    h, w = frame.shape[:2]\n",
        "\n",
        "    # translation matrices\n",
        "    tx, ty = current_transform\n",
        "    M_forward = np.float32([[1, 0, tx], [0, 1, ty]])\n",
        "    M_inverse = np.float32([[1, 0, -tx], [0, 1, -ty]])\n",
        "\n",
        "    # warping with border handling\n",
        "    warped_original = cv2.warpAffine(frame, M_inverse, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
        "    if blur_bg:\n",
        "        warped_original = cv2.GaussianBlur(warped_original, (blur_kernel, blur_kernel), 0)\n",
        "\n",
        "    # warping foreground RGBA (forward: subject to target) with transparent border\n",
        "    stabilized_fg_rgba = cv2.warpAffine(foreground_rgba, M_forward, (w, h), borderMode=cv2.BORDER_CONSTANT, borderValue=[0, 0, 0, 0])\n",
        "\n",
        "    # extracting for blending\n",
        "    alpha = stabilized_fg_rgba[:, :, 3].astype(np.float32) / 255.0\n",
        "    fg_bgr = stabilized_fg_rgba[:, :, 0:3].astype(np.float32)\n",
        "    bg_bgr = warped_original.astype(np.float32)\n",
        "\n",
        "    # soft alpha blending\n",
        "    blended = (1 - alpha[:, :, np.newaxis]) * bg_bgr + alpha[:, :, np.newaxis] * fg_bgr\n",
        "    stabilized_frame = np.clip(blended, 0, 255).astype(np.uint8)\n",
        "\n",
        "    if visualize:\n",
        "        cv2.circle(stabilized_frame, target_point, 10, (0, 255, 0), 2)  # target marker\n",
        "        for kp in keypoints_data:  # from pose detection\n",
        "            if kp[2] > 0.5:  # conf threshold\n",
        "                # applying forward transform to keypoints for correct position on stabilized frame\n",
        "                new_x = kp[0] + tx\n",
        "                new_y = kp[1] + ty\n",
        "                if 0 <= new_x < w and 0 <= new_y < h:  # clip to frame\n",
        "                    cv2.circle(stabilized_frame, (int(new_x), int(new_y)), 5, (255, 0, 0), -1)\n",
        "\n",
        "    return stabilized_frame\n",
        "\n",
        "def save_pose_data(poses_list, output_path):\n",
        "    \"\"\"\n",
        "    saving pose keypoints to JSON.\n",
        "    poses_list: list of {'frame': int, 'keypoints': list of [x, y, conf]}\n",
        "    \"\"\"\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(poses_list, f, indent=2)"
      ],
      "metadata": {
        "id": "rIX6ADIUKm2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile human-centric-video-stabilization/src/run.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "CLI entry point for human-centric video stabilization.\n",
        "usage: python src/run.py --input /path/to/video.mp4 [options]\n",
        "\"\"\"\n",
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "import torch  # for device check\n",
        "from background import load_background_remover, remove_background\n",
        "from pose import load_pose_model, detect_pose_center\n",
        "from stabilization import compute_transform, KalmanFilter\n",
        "from rendering import create_stabilized_frame, save_pose_data\n",
        "\n",
        "def process_video(input_path, output_stab=\"stabilized.mp4\", output_comp=\"comparison.mp4\",\n",
        "                  poses_path=\"poses.json\", target_x=None, target_y=None,\n",
        "                  device='cpu', blur_bg=False, visualize=False):\n",
        "    \"\"\"\n",
        "    Main processing pipeline.\n",
        "    \"\"\"\n",
        "    # detecting device if 'auto'\n",
        "    if device == 'auto':\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Auto-detected device: {device}\")\n",
        "\n",
        "    # loading models\n",
        "    print(\"Loading models...\")\n",
        "    model_bg, transform_bg = load_background_remover(device)\n",
        "    model_pose = load_pose_model(device=device)\n",
        "\n",
        "    # setup video\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Could not open video: {input_path}\")\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    target_point = (target_x or width // 2, target_y or height // 2)\n",
        "\n",
        "    # video writers\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out_stab = cv2.VideoWriter(output_stab, fourcc, fps, (width, height))\n",
        "    out_comp = cv2.VideoWriter(output_comp, fourcc, fps, (width * 2, height))\n",
        "\n",
        "    # init\n",
        "    kalman = KalmanFilter(process_noise=0.05, measurement_noise=0.5)  # tune as wish (hyperparam)\n",
        "    poses = []\n",
        "    frame_count = 0\n",
        "    start_time = time.time()\n",
        "    print(f\"Target point: {target_point}\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(\"Processing frames...\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # detecting pose and center\n",
        "        center_point, keypoints_data = detect_pose_center(model_pose, frame)\n",
        "\n",
        "        # computing transform\n",
        "        current_transform = compute_transform(center_point, target_point, kalman)\n",
        "\n",
        "        # removing background\n",
        "        foreground_rgba = remove_background(frame, model_bg, transform_bg)\n",
        "\n",
        "        # stabilizing and composite\n",
        "        stabilized_frame = create_stabilized_frame(frame, foreground_rgba, current_transform, blur_bg=blur_bg, visualize=visualize, keypoints_data=keypoints_data, target_point=target_point)\n",
        "\n",
        "        # saving pose data\n",
        "        poses.append({'frame': frame_count, 'keypoints': keypoints_data, 'transform': current_transform.tolist()})\n",
        "\n",
        "        # labels for comparison\n",
        "        orig_labeled = frame.copy()\n",
        "        stab_labeled = stabilized_frame.copy()\n",
        "        cv2.putText(orig_labeled, \"Original\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)\n",
        "        cv2.putText(stab_labeled, \"Stabilized\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)\n",
        "\n",
        "        comp_frame = np.hstack([orig_labeled, stab_labeled])\n",
        "\n",
        "        # write\n",
        "        out_stab.write(stabilized_frame)\n",
        "        out_comp.write(comp_frame)\n",
        "\n",
        "        frame_count += 1\n",
        "        if frame_count % 10 == 0:  # frequent updates for slow processing\n",
        "            print(f\"Processed {frame_count} frames...\")\n",
        "\n",
        "    # cleanup\n",
        "    cap.release()\n",
        "    out_stab.release()\n",
        "    out_comp.release()\n",
        "\n",
        "    # saving poses\n",
        "    save_pose_data(poses, poses_path)\n",
        "\n",
        "    # runtime\n",
        "    elapsed = time.time() - start_time\n",
        "    actual_fps = frame_count / elapsed if elapsed > 0 else 0\n",
        "    print(f\"✅ Complete! {frame_count} frames processed in {elapsed:.2f}s ({actual_fps:.2f} FPS).\")\n",
        "    print(f\"Stabilized: {output_stab}\")\n",
        "    print(f\"Comparison: {output_comp}\")\n",
        "    print(f\"Poses: {poses_path}\")\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Human-Centric Video Stabilization\")\n",
        "    parser.add_argument('--input', '-i', required=True, help=\"Input video path\")\n",
        "    parser.add_argument('--output-stab', '-s', default=\"stabilized.mp4\", help=\"Stabilized output path\")\n",
        "    parser.add_argument('--output-comp', '-c', default=\"comparison.mp4\", help=\"Comparison output path\")\n",
        "    parser.add_argument('--poses', '-p', default=\"poses.json\", help=\"Pose data output path\")\n",
        "    parser.add_argument('--target-x', type=int, help=\"Target X position (default: width/2)\")\n",
        "    parser.add_argument('--target-y', type=int, help=\"Target Y position (default: height/2)\")\n",
        "    parser.add_argument('--device', choices=['cpu', 'cuda', 'auto'], default='auto', help=\"Device (auto detects GPU if available)\")\n",
        "    parser.add_argument('--blur-bg', action='store_true', help=\"Blur background for emphasis\")\n",
        "    parser.add_argument('--visualize', action='store_true', help=\"Visualize keypoints and target point\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.input):\n",
        "        raise FileNotFoundError(f\"Input video not found: {args.input}\")\n",
        "\n",
        "    process_video(\n",
        "        input_path=args.input,\n",
        "        output_stab=args.output_stab,\n",
        "        output_comp=args.output_comp,\n",
        "        poses_path=args.poses,\n",
        "        target_x=args.target_x,\n",
        "        target_y=args.target_y,\n",
        "        device=args.device,\n",
        "        blur_bg=args.blur_bg,\n",
        "        visualize=args.visualize\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "acU8obNJKm4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd human-centric-video-stabilization && python src/run.py --input /content/8296233-hd_1080_1920_25fps.mp4 --output-stab /content/stabilized.mp4 --output-comp /content/comparison.mp4 --poses /content/poses.json --device auto --blur-bg --visualize"
      ],
      "metadata": {
        "id": "oyQbwAMaKm7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Erqne_-2Km9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLG_yMTYKnAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WxeDBnKGKnCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tTxnqkXBKnEy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}